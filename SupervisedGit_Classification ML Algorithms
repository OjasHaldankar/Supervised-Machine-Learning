#!/usr/bin/env python
# coding: utf-8


# Importing Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix


# Reading the dataset
df = pd.read_csv("P77_Profile Mapping.csv")
df


df['Category'].value_counts()


# Defining Feature (Independent Variable)
x = df['Skills']
cv = CountVectorizer()
X = cv.fit_transform(x)


# Defining Target Variable
df['Profile'] = df['Category'].map({'React / React Js Developer' : 0, 'Workday Consultant' : 1, 'Peoplesoft FSCM' : 2, 'Peoplesoft Admin' : 3, 'SQL Developer' : 4})
Y = df['Profile']

df[['Profile']]



# Label Encoding the Feature Variable for Classification 
from sklearn.preprocessing import LabelEncoder
values = np.array(df['Skills'])
le = LabelEncoder()
df['Skill_Encode'] = le.fit_transform(values)

df1 = df[['Skill_Encode', 'Profile']]
df1.head()


# Defining feature & target variable
array = df1.values
x1 = array[:, 0]
Y1 = array[:, 1]
X1 = x1.reshape(-1, 1)

# Splitting the dataset into Training & Testing Data
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state= 0)

# For Label Encoded 'Skills' data
X1_train, X1_test, Y1_train, Y1_test = train_test_split(X1, Y1, test_size=0.3, random_state= 10)


# ### **`ML Model [1]  :  Naive Bayes Classifier`** 

from sklearn.naive_bayes import MultinomialNB
NB = MultinomialNB()
NB.fit(X_train, Y_train)

Y_pred_test_NB = NB.predict(X_test)
Y_pred_test_NB

# Calculating Training Accuracy
Y_pred_train_NB = NB.predict(X_train)
Train_acc_NB = accuracy_score(Y_pred_train_NB, Y_train)*100
Train_acc_NB

# Calculating Testing Data Accuracy
Test_acc_NB = accuracy_score(Y_pred_test_NB, Y_test)*100
Test_acc_NB

# Classification Report
print(classification_report(Y_test, Y_pred_test_NB))


# ### **`ML Model [2]  :  K-Nearest Neighbors Classifier`**  

# Initializing KNN classifier
from sklearn.neighbors import KNeighborsClassifier

# Initialize the model by assuming k = 3
KNN = KNeighborsClassifier(n_neighbors = 3)
KNN.fit(X1_train, Y1_train)

# Calculating Training Accuracy
Y_pred_train_KNN = KNN.predict(X1_train)
Train_acc_KNN = accuracy_score(Y_pred_train_KNN, Y1_train)*100
Train_acc_KNN

# Calculating Testing Data Accuracy
Y_pred_test_KNN = KNN.predict(X1_test)
Test_acc_KNN = accuracy_score(Y_pred_test_KNN, Y1_test)*100
Test_acc_KNN


# **Finding the optimum value of 'K'**

# Initialize GridSearchCV
from sklearn.model_selection import GridSearchCV
n_neighbors = np.array(range(1,20))                #Finding optimum value of 'K' in the range of 1 to 20
param_grid = dict(n_neighbors = n_neighbors)

KNN = KNeighborsClassifier()
grid = GridSearchCV(estimator= KNN, param_grid = param_grid)
grid.fit(X1_train, Y1_train)

print(grid.best_params_)

Train_acc_Knn = grid.best_score_ *100
Train_acc_Knn

# Calculating Testing Data Accuracy
Y_pred_test_Knn = grid.predict(X1_test)
Test_acc_Knn = accuracy_score(Y_pred_test_Knn, Y1_test)*100
Test_acc_Knn

print(classification_report(Y1_test, Y_pred_test_Knn))


# ### **`ML Model [3]  :  Decision Tree Classifier`** 

# Initializing DT classifier
from sklearn.tree import  DecisionTreeClassifier
from sklearn import tree   

# Initialize the model
DT = DecisionTreeClassifier(criterion = 'entropy', max_depth=3)
DT.fit(X1_train,Y1_train) 

#PLot the decision tree
tree.plot_tree(DT);

fn = ['Skillset of Candidate']
cn = ['React / React Js Developer', 'Workday Consultant', 'Peoplesoft FSCM', 'Peoplesoft Admin', 'SQL Developer']
fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=300)
tree.plot_tree(DT,
               feature_names = fn, 
               class_names=cn,
               filled = True);      #filled = True means we want to fill boxes with colors

# Calculating Training Accuracy
Y_pred_train_DT = DT.predict(X1_train)
Train_acc_DT = accuracy_score(Y_pred_train_DT, Y1_train)*100
Train_acc_DT

#Predicting on test data
Y_pred_test_DT = DT.predict(X1_test)                       # predicting on test data set 
pd.Series(Y_pred_test_DT).value_counts()                   # getting the count of each category 

Y_pred_test_DT

# Calculating Testing Data Accuracy
Test_acc_DT =  np.mean(Y_pred_test_DT == Y1_test)*100
Test_acc_DT

print(classification_report(Y1_test, Y_pred_test_DT))


# ### **`ML Model [4]  :  Random Forest Classifier (Ensemble Modelling)`**  

from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestClassifier

# Initializing the RF Classifier
num_trees = 200
max_features = 1 
RF = RandomForestClassifier(n_estimators= num_trees, max_features= max_features)

RF.fit(X1_train, Y1_train)

# Calculating Training Accuracy
Y_pred_train_RF = RF.predict(X1_train)
Train_acc_RF = accuracy_score(Y_pred_train_RF, Y1_train)*100
Train_acc_RF

# Calculating Testing Data Accuracy
Y_pred_test_RF = RF.predict(X1_test)
Test_acc_RF = accuracy_score(Y_pred_test_RF, Y1_test)*100
Test_acc_RF

print(classification_report(Y1_test, Y_pred_test_RF))


# ### **`ML Model [5]  :  AdaBoost Classifier`** 

from sklearn.ensemble import AdaBoostClassifier
# Initializing the AdaBoost Classifier
num_trees = 200
seed = 5 
AB = AdaBoostClassifier(n_estimators=num_trees, random_state = seed) 

AB.fit(X1_train, Y1_train)

# Calculating Training Accuracy
Y_pred_train_AB = AB.predict(X1_train)
Train_acc_AB = accuracy_score(Y_pred_train_AB, Y1_train)*100
Train_acc_AB

# Calculating Testing Data Accuracy
Y_pred_test_AB = AB.predict(X1_test)
Test_acc_AB = accuracy_score(Y_pred_test_AB, Y1_test)*100
Test_acc_AB


# ### **`ML Model [6]  :  Support Vector Classifier`** 

from sklearn import svm
from sklearn.svm import SVC                                 
from sklearn.model_selection import GridSearchCV

# Initializing SVC
svc = SVC()
svc.fit(X1_train, Y1_train)

# Calculating Training Accuracy
Y_pred_train_SVC = svc.predict(X1_train)
Train_acc_SVC = accuracy_score(Y_pred_train_SVC, Y1_train)*100
Train_acc_SVC

# Calculating Testing Data Accuracy
Y_pred_test_SVC = svc.predict(X1_test)
Test_acc_SVC = accuracy_score(Y_pred_test_SVC, Y1_test)*100
Test_acc_SVC

print(classification_report(Y1_test, Y_pred_test_SVC))


# ### **`ML Model [7]  :  Artificial Neural Network`** 

from keras.models import Sequential
from keras.layers import Dense

# for maintaining the accuracy of the result, creating seed (random state)
seed = 1
np.random.seed(seed)

from keras.wrappers.scikit_learn import KerasClassifier
from keras.optimizers import adam_v2
from sklearn.model_selection import GridSearchCV, KFold


# Hypertuning of Batchsize & Epochs
# create model
def create_model():
    model = Sequential()
    model.add(Dense(15, input_dim=1 ,activation='relu'))
    model.add(Dense(10,input_dim=1, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    
    model.compile(loss='binary_crossentropy', metrics=['accuracy'])
    return model


# Building the model
model = KerasClassifier(build_fn = create_model, verbose = 0)
# Grid search parameters
batch_size = [10,20,40]
epochs = [10,50,100]
# Make a dictionary of the grid search parameters
param_grid = dict(batch_size = batch_size,epochs = epochs)
# Build and fit the GridSearchCV
grid = GridSearchCV(estimator = model, param_grid = param_grid, cv = KFold(), verbose = 10)
grid_result = grid.fit(X1_train, Y1_train) 

# Summarize the results
print('Best : {}, using {}'.format(grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
  print('{},{} with: {}'.format(mean, stdev, param))


# Hypertuning of Activation Function & Kernal Initializer
from keras.layers import Dropout
def create_model(activation_function,init):
    model = Sequential()
    model.add(Dense(8,input_dim = 1,kernel_initializer = init,activation = activation_function))
    model.add(Dropout(0.1))
    model.add(Dense(4,input_dim = 1,kernel_initializer = init,activation = activation_function))
    model.add(Dropout(0.1))
    model.add(Dense(1,activation = 'sigmoid'))
    
    model.compile(loss = 'binary_crossentropy', metrics = ['accuracy'])
    return model
# Create the model
model = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 50,epochs = 10)
# Define the grid search parameters
activation_function = ['softmax','relu','tanh','linear']
init = ['uniform','normal','zero']
# Make a dictionary of the grid search parameters
param_grids = dict(activation_function = activation_function,init = init)
# Build and fit the GridSearchCV
grid = GridSearchCV(estimator = model,param_grid = param_grids,cv = KFold(),verbose = 10)
grid_result = grid.fit(X1_train, Y1_train)

# Summarize the results
print('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
  print('{},{} with: {}'.format(mean, stdev, param)) 


# Hypertuning of number of neurons in Activation Layer
def create_model(neuron1,neuron2):
    model = Sequential()
    model.add(Dense(neuron1,input_dim = 1,kernel_initializer = 'uniform',activation = 'relu'))
    model.add(Dropout(0.1))
    model.add(Dense(neuron2,input_dim = neuron1,kernel_initializer = 'uniform',activation = 'relu'))
    model.add(Dropout(0.1))
    model.add(Dense(1,activation = 'sigmoid'))
    
    model.compile(loss = 'binary_crossentropy', metrics = ['accuracy'])
    return model
# Create the model
model = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 40,epochs = 10)
# Define the grid search parameters
neuron1 = [4,8,16]
neuron2 = [2,4,8]
# Make a dictionary of the grid search parameters
param_grids = dict(neuron1 = neuron1,neuron2 = neuron2)
# Build and fit the GridSearchCV
grid = GridSearchCV(estimator = model,param_grid = param_grids,cv = KFold(),verbose = 10)
grid_result = grid.fit(X1_train, Y1_train) 

# Summarize the results
print('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
  print('{},{} with: {}'.format(mean, stdev, param))


# Training the model with optimum values
from sklearn.metrics import classification_report, accuracy_score
# Defining the model
def create_model():
    model = Sequential()
    model.add(Dense(4,input_dim = 1,kernel_initializer = 'zero',activation = 'softmax'))
    model.add(Dropout(0.1))
    model.add(Dense(2,input_dim = 1,kernel_initializer = 'zero',activation = 'softmax'))
    model.add(Dropout(0.1))
    model.add(Dense(1,activation = 'sigmoid'))
    
    # adam = Adam(lr = 0.01) #sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)
    model.compile(loss = 'binary_crossentropy', metrics = ['accuracy'])
    return model
# Create the model
model = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 10, epochs = 10)
# Fitting the model
model.fit(X1_train,Y1_train)
# Predicting using trained model
Y_pred_test_ANN = model.predict(X1_test)
# Printing the metrics
Test_acc_ANN = accuracy_score(Y1_test, Y_pred_test_ANN)*100
Test_acc_ANN


Y_pred_train_ANN = model.predict(X1_train)
Train_acc_ANN = accuracy_score(Y1_train, Y_pred_train_ANN)*100
Train_acc_ANN


# ## **`Compairing the results of all algorithms`** 

Result = {'ML Algorithm':['Naive Bayes','KNN','Decision Tree', 'Random Forest', 'AdaBoost', 'SVM', 'ANN' ],
    'Train Accuracy':[Train_acc_NB, Train_acc_Knn, Train_acc_DT, Train_acc_RF, Train_acc_AB, Train_acc_SVC, Train_acc_ANN]
    ,'Test Accuracy':[Test_acc_NB, Test_acc_Knn, Test_acc_DT, Test_acc_RF, Test_acc_AB, Test_acc_SVC, Test_acc_ANN]}

Comp_table = pd.DataFrame(Result, index = [1,2,3,4,5,6,7])
Comp_table

Comp_table.sort_values(by= ['Test Accuracy'], ascending= False)

plt.figure(figsize =(20, 6))
plt.plot(Comp_table['ML Algorithm'], Comp_table['Train Accuracy'])
plt.plot(Comp_table['ML Algorithm'], Comp_table['Test Accuracy'])
plt.title('Model Accuracies',fontdict={'fontsize': 25,'fontweight' : 15,'color' : 'g'})
plt.xlabel('ML Algorithm')
plt.ylabel('Accuracy')
plt.legend(['Train Acc', 'Test Acc'], loc='lower right')
plt.grid()
plt.show();

